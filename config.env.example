# =============================================================================
# GGUF Experiments - Configuration File
# =============================================================================
# Copy this file to config.env and customize for your system:
#   cp config.env.example config.env
#
# All settings have sensible defaults optimized for larger-than-RAM inference.
# =============================================================================

# -----------------------------------------------------------------------------
# MODEL SETTINGS
# -----------------------------------------------------------------------------
# ⚠️  IMPORTANT: If you get a "Repository Not Found" or "Entry Not Found" error,
#     you need to update HF_REPO and MODEL_QUANT below to valid values.
#
# How to find a valid GGUF repository:
#   1. Search HuggingFace: https://huggingface.co/models?search=llama-4-scout+gguf
#   2. Look for repos from trusted quantizers like:
#      - unsloth (https://huggingface.co/unsloth)
#      - lmstudio-community (https://huggingface.co/lmstudio-community)
#      - bartowski (https://huggingface.co/bartowski)
#      - TheBloke (https://huggingface.co/TheBloke)
#   3. Copy the repository name (e.g., "unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF")
#   4. Check the "Files" tab for available quantizations (Q8_0, Q4_K_M, etc.)
#   5. Update HF_REPO and MODEL_QUANT below
#
# Official Llama 4 Scout model (safetensors, not GGUF):
#   https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct
# -----------------------------------------------------------------------------

# Path to your GGUF model file (set automatically by 'make download')
# For sharded models, this points to the first shard; llama.cpp loads all parts
MODEL_PATH=./models/Q8_0/Llama-4-Scout-17B-16E-Instruct-Q8_0-00001-of-00003.gguf

# HuggingFace repository for the GGUF model
# ⚠️  CHANGE THIS if you get "Repository Not Found" errors!
# 
# Common GGUF repositories to try (verify availability on HuggingFace):
#   - unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF
#   - lmstudio-community/Llama-4-Scout-17B-16E-Instruct-GGUF
#   - bartowski/Llama-4-Scout-17B-16E-Instruct-GGUF
#
# To verify a repo exists, visit: https://huggingface.co/<REPO_NAME>
HF_REPO=unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF

# Model quantization level to download
# ⚠️  CHANGE THIS to match an available quantization in your chosen HF_REPO!
#
# Check available options at: https://huggingface.co/<HF_REPO>/tree/main
#
# SHARDED MODELS (stored in subdirectories, multiple files):
# These are split across multiple .gguf files for easier downloading.
# The download script will fetch all parts automatically.
#   - Q8_0     (115 GB, 8-bit, near-lossless quality) ← DEFAULT
#   - Q6_K     (88.4 GB, 6-bit, excellent quality)
#   - Q5_K_M   (76.5 GB, 5-bit, very good quality)
#   - Q5_K_S   (74.3 GB, 5-bit, good quality)
#   - Q4_K_M   (65.4 GB, 4-bit, good quality/size balance)
#   - Q4_K_S   (61.5 GB, 4-bit, smaller)
#   - Q3_K_M   (51.8 GB, 3-bit, acceptable quality)
#   - BF16     (216 GB, bfloat16, highest quality)
#
# SINGLE-FILE MODELS (stored at root level):
# These are smaller quantizations in a single file.
#   - Q3_K_S   (46.7 GB, 3-bit, smaller single file)
#   - Q2_K     (39.6 GB, 2-bit, smallest, lower quality)
#   - Q2_K_L   (39.8 GB, 2-bit large, slightly better than Q2_K)
MODEL_QUANT=Q8_0

# Model name prefix (used to construct file paths)
# Usually matches the model name without the quantization suffix
MODEL_NAME=Llama-4-Scout-17B-16E-Instruct

# -----------------------------------------------------------------------------
# DOWNLOAD SETTINGS
# -----------------------------------------------------------------------------
# These settings control how model files are downloaded from HuggingFace Hub.
# Useful for slow or unstable connections when downloading 50GB+ files.

# Timeout for each download request in seconds (default: 60 minutes)
# Increase this if you have a slow connection
DOWNLOAD_TIMEOUT=3600

# Maximum number of retry attempts if download fails (default: 5)
DOWNLOAD_MAX_RETRIES=5

# Initial delay between retries in seconds (doubles with each retry, max 5 min)
DOWNLOAD_RETRY_DELAY=10

# hf_transfer for faster downloads (auto-detected, no config needed)
# The download script automatically enables hf_transfer if available.
# hf_transfer uses Rust-based parallel downloads for 5-10x speedup.
# It's included in dependencies and installed via: uv sync

# -----------------------------------------------------------------------------
# MEMORY SETTINGS (Important for larger-than-RAM inference!)
# -----------------------------------------------------------------------------
# How larger-than-RAM inference works:
#   1. The model file is memory-mapped (mmap), NOT loaded entirely into RAM
#   2. The OS pages model weights in/out of RAM as needed during inference
#   3. The KV cache (attention state) is allocated in RAM based on context size
#
# There is NO setting to hard-limit total RAM usage. The OS manages memory
# automatically via paging. To reduce memory pressure:
#   - Use a smaller CONTEXT_SIZE (directly reduces KV cache memory)
#   - Use quantized KV cache types (KV_CACHE_TYPE_K/V = q8_0 or q4_0)
#   - Use a smaller BATCH_SIZE
#   - Close other applications to free RAM for paging
# -----------------------------------------------------------------------------

# Enable memory mapping (mmap) - ESSENTIAL for larger-than-RAM models
# When enabled, the OS pages model data in/out of RAM as needed
# This is enabled by default in llama.cpp; we set it explicitly for clarity
USE_MMAP=true

# Lock model in RAM (mlock) - prevents swapping, requires model to fit in RAM
# Set to "false" for larger-than-RAM operation (REQUIRED)
# Set to "true" only if the model fits entirely in your RAM
USE_MLOCK=false

# KV cache data type for keys - quantized types reduce memory significantly
# Options: f16 (default), f32, q8_0 (50% less memory), q4_0 (75% less memory)
# Using q8_0 is recommended for larger-than-RAM operation with minimal quality loss
KV_CACHE_TYPE_K=q8_0

# KV cache data type for values - same options as above
KV_CACHE_TYPE_V=q8_0

# -----------------------------------------------------------------------------
# INFERENCE SETTINGS
# -----------------------------------------------------------------------------

# Context window size (number of tokens)
# Larger = more conversation history, but MORE MEMORY (KV cache scales with this)
# Llama 4 Scout supports up to 131072 tokens, but use smaller for less RAM
# Memory impact: 2048 tokens ≈ 1-2GB KV cache, 8192 tokens ≈ 4-8GB KV cache
CONTEXT_SIZE=2048

# Number of tokens to predict (-1 for unlimited until stop token)
MAX_TOKENS=1024

# Temperature for generation (0.0 = deterministic, 1.0 = creative)
TEMPERATURE=0.7

# Number of threads to use for CPU inference
# Default: auto-detect based on CPU cores
THREADS=auto

# Batch size for prompt processing
# Larger = faster prompt processing, but more working memory
# Reduce this if you experience memory pressure during long prompts
BATCH_SIZE=256

# -----------------------------------------------------------------------------
# SERVER SETTINGS (for OpenAI-compatible API)
# -----------------------------------------------------------------------------

# Host to bind the server to
SERVER_HOST=127.0.0.1

# Port for the API server
SERVER_PORT=8080

# Enable verbose server logging
SERVER_VERBOSE=false

# -----------------------------------------------------------------------------
# GPU SETTINGS
# -----------------------------------------------------------------------------

# Number of layers to offload to GPU
# Default is 0 (CPU-only) for maximum compatibility across machines.
# Set to 999 to offload all layers to GPU (requires Metal on Apple Silicon).
# Note: On Apple Silicon, GPU and CPU share the same RAM!
GPU_LAYERS=0

# For GPU acceleration on Apple Silicon, uncomment this:
# GPU_LAYERS=999

# -----------------------------------------------------------------------------
# CHAT SETTINGS
# -----------------------------------------------------------------------------

# System prompt for chat mode
SYSTEM_PROMPT="You are a helpful AI assistant. Be concise and accurate."

# Chat prompt template (model-specific, Llama 4 Instruct format)
# Leave empty to use llama.cpp's auto-detection
CHAT_TEMPLATE=

# -----------------------------------------------------------------------------
# ADVANCED SETTINGS
# -----------------------------------------------------------------------------

# Repeat penalty (reduces repetition in outputs)
REPEAT_PENALTY=1.1

# Top-K sampling (0 = disabled)
TOP_K=40

# Top-P (nucleus) sampling
TOP_P=0.95

# Enable flash attention (faster, requires compatible hardware)
FLASH_ATTENTION=true
