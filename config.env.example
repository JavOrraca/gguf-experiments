# =============================================================================
# GGUF Experiments - Configuration File
# =============================================================================
# Copy this file to config.env and customize for your system:
#   cp config.env.example config.env
#
# All settings have sensible defaults, but you should adjust RAM_LIMIT
# based on your available memory.
# =============================================================================

# -----------------------------------------------------------------------------
# MODEL SETTINGS
# -----------------------------------------------------------------------------

# Path to your GGUF model file
# After running 'make download', this will be set automatically
MODEL_PATH=./models/Llama-4-Scout-17B-16E-Instruct-Q4_K_M.gguf

# HuggingFace repository for the GGUF model
# Change this if using a different quantization or model
HF_REPO=bartowski/Llama-4-Scout-17B-16E-Instruct-GGUF

# Specific model file to download
MODEL_FILE=Llama-4-Scout-17B-16E-Instruct-Q4_K_M.gguf

# -----------------------------------------------------------------------------
# MEMORY SETTINGS (Important for larger-than-RAM inference!)
# -----------------------------------------------------------------------------

# Maximum RAM to use for the model (in bytes, or use G suffix for gigabytes)
# This is the KEY setting for larger-than-RAM operation
#
# Recommendations:
#   - 8GB RAM system:  RAM_LIMIT=5G
#   - 16GB RAM system: RAM_LIMIT=10G
#   - 24GB RAM system: RAM_LIMIT=16G
#   - 32GB RAM system: RAM_LIMIT=24G
#
# Setting this LOWER than your model size enables memory-mapped inference,
# where parts of the model are loaded on-demand from disk (slower but works!)
RAM_LIMIT=16G

# Enable memory mapping (mmap) - ESSENTIAL for larger-than-RAM models
# When enabled, the OS will page model data in/out of RAM as needed
# Set to "true" to enable (default), "false" to disable
USE_MMAP=true

# Lock model in RAM (mlock) - prevents swapping, but requires enough RAM
# Set to "false" for larger-than-RAM operation (default)
# Set to "true" if model fits in RAM and you want consistent performance
USE_MLOCK=false

# -----------------------------------------------------------------------------
# INFERENCE SETTINGS
# -----------------------------------------------------------------------------

# Context window size (number of tokens)
# Larger = more conversation history, but uses more memory
# Llama 4 Scout supports up to 131072 tokens, but start smaller
CONTEXT_SIZE=4096

# Number of tokens to predict (-1 for unlimited until stop token)
MAX_TOKENS=2048

# Temperature for generation (0.0 = deterministic, 1.0 = creative)
TEMPERATURE=0.7

# Number of threads to use for CPU inference
# Default: auto-detect based on CPU cores
# For M1/M2/M3 Macs, llama.cpp will use Metal GPU automatically
THREADS=auto

# Batch size for prompt processing
# Larger = faster prompt processing, more memory
BATCH_SIZE=512

# -----------------------------------------------------------------------------
# SERVER SETTINGS (for OpenAI-compatible API)
# -----------------------------------------------------------------------------

# Host to bind the server to
SERVER_HOST=127.0.0.1

# Port for the API server
SERVER_PORT=8080

# Enable verbose server logging
SERVER_VERBOSE=false

# -----------------------------------------------------------------------------
# GPU SETTINGS (Apple Silicon / Metal)
# -----------------------------------------------------------------------------

# Number of layers to offload to GPU (Metal on Apple Silicon)
# Set to a high number to offload as many as possible
# Set to 0 for CPU-only inference
# Note: On Apple Silicon, GPU and CPU share the same RAM!
GPU_LAYERS=999

# For true CPU-only operation, uncomment this:
# GPU_LAYERS=0

# -----------------------------------------------------------------------------
# CHAT SETTINGS
# -----------------------------------------------------------------------------

# System prompt for chat mode
SYSTEM_PROMPT="You are a helpful AI assistant. Be concise and accurate."

# Chat prompt template (model-specific, Llama 4 Instruct format)
# Leave empty to use llama.cpp's auto-detection
CHAT_TEMPLATE=

# -----------------------------------------------------------------------------
# ADVANCED SETTINGS
# -----------------------------------------------------------------------------

# Repeat penalty (reduces repetition in outputs)
REPEAT_PENALTY=1.1

# Top-K sampling (0 = disabled)
TOP_K=40

# Top-P (nucleus) sampling
TOP_P=0.95

# Enable flash attention (faster, requires compatible hardware)
FLASH_ATTENTION=true
