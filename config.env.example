# =============================================================================
# GGUF Experiments - Configuration File
# =============================================================================
# Copy this file to config.env and customize for your system:
#   cp config.env.example config.env
#
# All settings have sensible defaults, but you should adjust RAM_LIMIT
# based on your available memory.
# =============================================================================

# -----------------------------------------------------------------------------
# MODEL SETTINGS
# -----------------------------------------------------------------------------
# ⚠️  IMPORTANT: If you get a "Repository Not Found" or "Entry Not Found" error,
#     you need to update HF_REPO and MODEL_QUANT below to valid values.
#
# How to find a valid GGUF repository:
#   1. Search HuggingFace: https://huggingface.co/models?search=llama-4-scout+gguf
#   2. Look for repos from trusted quantizers like:
#      - unsloth (https://huggingface.co/unsloth)
#      - lmstudio-community (https://huggingface.co/lmstudio-community)
#      - bartowski (https://huggingface.co/bartowski)
#      - TheBloke (https://huggingface.co/TheBloke)
#   3. Copy the repository name (e.g., "unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF")
#   4. Check the "Files" tab for available quantizations (Q8_0, Q4_K_M, etc.)
#   5. Update HF_REPO and MODEL_QUANT below
#
# Official Llama 4 Scout model (safetensors, not GGUF):
#   https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct
# -----------------------------------------------------------------------------

# Path to your GGUF model file (set automatically by 'make download')
# For sharded models, this points to the first shard; llama.cpp loads all parts
MODEL_PATH=./models/Q8_0/Llama-4-Scout-17B-16E-Instruct-Q8_0-00001-of-00003.gguf

# HuggingFace repository for the GGUF model
# ⚠️  CHANGE THIS if you get "Repository Not Found" errors!
# 
# Common GGUF repositories to try (verify availability on HuggingFace):
#   - unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF
#   - lmstudio-community/Llama-4-Scout-17B-16E-Instruct-GGUF
#   - bartowski/Llama-4-Scout-17B-16E-Instruct-GGUF
#
# To verify a repo exists, visit: https://huggingface.co/<REPO_NAME>
HF_REPO=unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF

# Model quantization level to download
# ⚠️  CHANGE THIS to match an available quantization in your chosen HF_REPO!
#
# Check available options at: https://huggingface.co/<HF_REPO>/tree/main
#
# SHARDED MODELS (stored in subdirectories, multiple files):
# These are split across multiple .gguf files for easier downloading.
# The download script will fetch all parts automatically.
#   - Q8_0     (115 GB, 8-bit, near-lossless quality) ← DEFAULT
#   - Q6_K     (88.4 GB, 6-bit, excellent quality)
#   - Q5_K_M   (76.5 GB, 5-bit, very good quality)
#   - Q5_K_S   (74.3 GB, 5-bit, good quality)
#   - Q4_K_M   (65.4 GB, 4-bit, good quality/size balance)
#   - Q4_K_S   (61.5 GB, 4-bit, smaller)
#   - Q3_K_M   (51.8 GB, 3-bit, acceptable quality)
#   - BF16     (216 GB, bfloat16, highest quality)
#
# SINGLE-FILE MODELS (stored at root level):
# These are smaller quantizations in a single file.
#   - Q3_K_S   (46.7 GB, 3-bit, smaller single file)
#   - Q2_K     (39.6 GB, 2-bit, smallest, lower quality)
#   - Q2_K_L   (39.8 GB, 2-bit large, slightly better than Q2_K)
MODEL_QUANT=Q8_0

# Model name prefix (used to construct file paths)
# Usually matches the model name without the quantization suffix
MODEL_NAME=Llama-4-Scout-17B-16E-Instruct

# -----------------------------------------------------------------------------
# DOWNLOAD SETTINGS
# -----------------------------------------------------------------------------
# These settings control how model files are downloaded from HuggingFace Hub.
# Useful for slow or unstable connections when downloading 50GB+ files.

# Timeout for each download request in seconds (default: 60 minutes)
# Increase this if you have a slow connection
DOWNLOAD_TIMEOUT=3600

# Maximum number of retry attempts if download fails (default: 5)
DOWNLOAD_MAX_RETRIES=5

# Initial delay between retries in seconds (doubles with each retry, max 5 min)
DOWNLOAD_RETRY_DELAY=10

# hf_transfer for faster downloads (auto-detected, no config needed)
# The download script automatically enables hf_transfer if available.
# hf_transfer uses Rust-based parallel downloads for 5-10x speedup.
# It's included in dependencies and installed via: uv sync

# -----------------------------------------------------------------------------
# MEMORY SETTINGS (Important for larger-than-RAM inference!)
# -----------------------------------------------------------------------------

# Maximum RAM to use for the model (in bytes, or use G suffix for gigabytes)
# This is the KEY setting for larger-than-RAM operation
#
# Recommendations (set to ~50% of your total RAM):
#   - 8GB RAM system:  RAM_LIMIT=4G
#   - 16GB RAM system: RAM_LIMIT=8G
#   - 24GB RAM system: RAM_LIMIT=12G
#   - 32GB RAM system: RAM_LIMIT=16G
#
# Setting this LOWER than your model size enables memory-mapped inference,
# where parts of the model are loaded on-demand from disk (slower but works!)
RAM_LIMIT=12G

# Enable memory mapping (mmap) - ESSENTIAL for larger-than-RAM models
# When enabled, the OS will page model data in/out of RAM as needed
# Set to "true" to enable (default), "false" to disable
USE_MMAP=true

# Lock model in RAM (mlock) - prevents swapping, but requires enough RAM
# Set to "false" for larger-than-RAM operation (default)
# Set to "true" if model fits in RAM and you want consistent performance
USE_MLOCK=false

# -----------------------------------------------------------------------------
# INFERENCE SETTINGS
# -----------------------------------------------------------------------------

# Context window size (number of tokens)
# Larger = more conversation history, but uses more memory
# Llama 4 Scout supports up to 131072 tokens, but start smaller
CONTEXT_SIZE=4096

# Number of tokens to predict (-1 for unlimited until stop token)
MAX_TOKENS=2048

# Temperature for generation (0.0 = deterministic, 1.0 = creative)
TEMPERATURE=0.7

# Number of threads to use for CPU inference
# Default: auto-detect based on CPU cores
# For M1/M2/M3 Macs, llama.cpp will use Metal GPU automatically
THREADS=auto

# Batch size for prompt processing
# Larger = faster prompt processing, more memory
BATCH_SIZE=512

# -----------------------------------------------------------------------------
# SERVER SETTINGS (for OpenAI-compatible API)
# -----------------------------------------------------------------------------

# Host to bind the server to
SERVER_HOST=127.0.0.1

# Port for the API server
SERVER_PORT=8080

# Enable verbose server logging
SERVER_VERBOSE=false

# -----------------------------------------------------------------------------
# GPU SETTINGS (Apple Silicon / Metal)
# -----------------------------------------------------------------------------

# Number of layers to offload to GPU (Metal on Apple Silicon)
# Set to a high number to offload as many as possible
# Set to 0 for CPU-only inference
# Note: On Apple Silicon, GPU and CPU share the same RAM!
GPU_LAYERS=999

# For true CPU-only operation, uncomment this:
# GPU_LAYERS=0

# -----------------------------------------------------------------------------
# CHAT SETTINGS
# -----------------------------------------------------------------------------

# System prompt for chat mode
SYSTEM_PROMPT="You are a helpful AI assistant. Be concise and accurate."

# Chat prompt template (model-specific, Llama 4 Instruct format)
# Leave empty to use llama.cpp's auto-detection
CHAT_TEMPLATE=

# -----------------------------------------------------------------------------
# ADVANCED SETTINGS
# -----------------------------------------------------------------------------

# Repeat penalty (reduces repetition in outputs)
REPEAT_PENALTY=1.1

# Top-K sampling (0 = disabled)
TOP_K=40

# Top-P (nucleus) sampling
TOP_P=0.95

# Enable flash attention (faster, requires compatible hardware)
FLASH_ATTENTION=true
